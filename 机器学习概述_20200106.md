# 综述

提纲：  
一、机器学习定义  
二、机器学习分类  
三、机器学习模型  
损失函数/数据集的划分/过拟合和欠拟合/优化方法/评价指标等    


## 一、机器学习定义：
机器学习是利用计算机算法和统计模型是计算机系统使用，逐步提高完成特定任务的能力。  
用过去的历史数据预测未来。

## 二、机器学习分类：
### 根据学习方式的差异，机器学习可以分为四种：
1.监督学习：有目标变量。监督的学习分为回归和分类。  
        回归-连续-最小二乘，平方损失。  
        分类-离散-交叉熵，0-1损失。  
2.无监督学习：没有目标变量。无须对数据集进行标记，即没有输出。  
3.半监督学习：监督学习和非监督学习的结合，数据集一部分有target。  
4.强化学习：是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题。  
### 基于学习目标，可分为五类:
1.概念学习  
2.规则学习  
3.函数学习  
4.类别学习  
5.贝叶斯网络学习  

## 三、机器学习模型
机器学习 = 数据（data） + 模型（model） + 优化方法（optimal strategy）  

### 1.数据集划分：train/test/valid  
train：训练数据集  
test：测试模型好坏  
valid：同样用来测试模型好坏，和test的区别是，valid更新和调整模型参数，选择最优。  
交叉验证：由于总的数据集数量是有限的，如果把数据集完全独立的分成三块，会减少model的数据量，也会使valid的调整参数的数据量变少。所以在训练集中不想吧valid的数据集丢掉，就需要交叉验证。  
eg：将train数据集分为a/b/c/d/e五份，  
首先，train b/c/d/e，valid a，得到acc 1；  
然后，train a/c/d/e，valid b，得到acc 2；  
然后，train a/b/d/e，valid c，得到acc 3；  
然后，train a/b/c/e，valid d，得到acc 4；  
然后，train a/b/c/d，valid e，得到acc 5；  
最后，acc = (acc1+acc2+acc3+acc4+acc5)/5 为最终最优参数值。  
另外有一种方法，“留一”：假设有1000个样本，用999个建模，用第1个样本验证，然后用第2个样本验证，剩下的999个建模，以此类推，一共建模1000次，非常耗时。  
这个又叫作k-折叠交叉验证，  
假设训练集为S ，将训练集等分为k份:${S_1, S_2, ..., S_k}$.  
然后每次从集合中拿出k-1份进行训练，  
利用集合中剩下的那一份来进行测试并计算损失值，  
最后得到k次测试得到的损失值，并选择平均损失值最小的模型。    

### 2.损失函数  
（1）0-1损失函数 $$ L(y,f(x)) = \begin{cases} 0, & \text{y = f(x)} \ 1, & \text{y $\neq$ f(x)} \end{cases} $$  
（2）绝对值损失函数 $$ L(y,f(x))=|y-f(x)| $$  
（3）平方损失函数 $$ L(y,f(x))=(y-f(x))^2 $$  
（4）log对数损失函数 $$ L(y,f(x))=log(1+e^{-yf(x)}) $$  
（5）指数损失函数 $$ L(y,f(x))=exp(-yf(x)) $$  
（6）Hinge损失函数 $$ L(w,b)=max{0,1-yf(x)} $$    
以线性回归为例，Y = b0 + b1*X , 样本空间可以确定无数个这样的方程，我们需要从这些方程中选出一个最优解。最小二乘法。凸优化。  
我们根据历史数据建立模型，目标就是在优化代价函数，不同的模型就有不同的代价函数。

### 3.模型的选择：过拟合和欠拟合
线性回归为例，同一个样本拟合出一下三个模型，  
M1:  Y1 = b0 + b1*X  
M2:  Y2 = b0 + b1*X + b2*X^2  
M3:  Y3 = b0 + b1*X + b2*X^2 + b3*X^3 + b4*X^4  
直观地，损失函数值达到最小，M3代价最小，M1代价最大。M3的每个点都在直线上，所以M3过拟合了。  
过拟合Overfitting：特征过多，能够非常好地拟合训练数据集，但是不能在测试集上效果达到最佳，不能很好的预测未来，也就是没有较好的泛化能力。  
欠拟合：M1.  

模型的复杂度，  
训练误差，复杂度越高，误差越小。  
测试误差，一开始，复杂度越高，误差越小，但是到了后面，复杂度更高的时候，误差变大。  
训练模型的目的，我们是希望训练误差和测试误差都达到最小，更多的是希望测试误差达到较小，因为测试集是未知的数据，而我们要追求模型的泛化，就是模型在未知数据上的表现能力。  

### 4.避免过拟合：  
（1）减少特征数：  
a. 人工的方法，根据经验删除和保留  
b. Model based，用模型的方法。  
（2）Regularization，正则项或者正则化。正则会保留所有的特征，它会尽量让每个feature的影响达到比较小的程度。  
代价函数：  
要使整个代价函数的值最小，就要让theta3和theta4的值趋向最小，让特征趋向于0。
正则会让theta变小，就不会过拟合。 过拟合就是因为特征过大。  
一般，我们会惩罚所有的theta，通过lambda参数，它是权衡的作用，控制两个目标中的平衡关系，第一个是使假设更好地拟合训练数据，是训练误差达到最小，第二是要保持参数较小，在两者之前权衡，lambda的目的也是避免过拟合。  

### 5.参数调优：
（1）网格搜索  
一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果。  
（2）随机搜索：  
与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。  
（3）贝叶斯优化算法：  
贝叶斯优化用于机器学习调参由J.Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。  