综述

提纲：
（一）机器学习介绍，略。
（二）机器学习分类
（三）机器学习方法三要素


（一）机器学习介绍：
用过去的历史数据预测未来。

（二）机器学习分类：
监督学习：有目标变量。监督的学习分为回归和分类。
        回归-连续-最小二乘，平方损失。
        分类-离散-交叉熵，0-1损失。
无监督学习：没有目标变量
半监督学习：数据集一部分有target

（三）机器学习模型：
1.数据集划分：train/test/valid
train：训练数据集
test：测试模型好坏
valid：同样用来测试模型好坏，和test的区别是，valid更新和调整模型参数，选择最优。
交叉验证：由于总的数据集数量是有限的，如果把数据集完全独立的分成三块，会减少model的数据量，也会使valid的调整参数的数据量变少。所以在训练集中不想吧valid的数据集丢掉，就需要交叉验证。
eg：将train数据集分为a/b/c/d/e五份，
首先，train b/c/d/e，valid a，得到acc 1；
然后，train a/c/d/e，valid b，得到acc 2；
然后，train a/b/d/e，valid c，得到acc 3；
然后，train a/b/c/e，valid d，得到acc 4；
然后，train a/b/c/d，valid e，得到acc 5；
最后，acc = (acc1+acc2+acc3+acc4+acc5)/5 为最终最优参数值。
另外有一种方法，“留一”：假设有1000个样本，用999个建模，用第1个样本验证，然后用第2个样本验证，剩下的999个建模，以此类推，一共建模1000次，非常耗时。

2.损失函数：
以线性回归为例，Y = b0 + b1*X , 样本空间可以确定无数个这样的方程，我们需要从这些方程中选出一个最优解。最小二乘法。凸优化。
我们根据历史数据建立模型，目标就是在优化代价函数，不同的模型就有不同的代价函数。

3.模型的选择：过拟合和欠拟合
线性回归为例，同一个样本拟合出一下三个模型，
M1:  Y1 = b0 + b1*X
M2:  Y2 = b0 + b1*X + b2*X^2
M3:  Y3 = b0 + b1*X + b2*X^2 + b3*X^3 + b4*X^4
直观地，损失函数值达到最小，M3代价最小，M1代价最大。M3的每个点都在直线上，所以M3过拟合了。
过拟合Overfitting：特征过多，能够非常好地拟合训练数据集，但是不能在测试集上效果达到最佳，不能很好的预测未来，也就是没有较好的泛化能力。
欠拟合：M1.

模型的复杂度，
训练误差，复杂度越高，误差越小。
测试误差，一开始，复杂度越高，误差越小，但是到了后面，复杂度更高的时候，误差变大。
训练模型的目的，我们是希望训练误差和测试误差都达到最小，更多的是希望测试误差达到较小，因为测试集是未知的数据，而我们要追求模型的泛化，就是模型在未知数据上的表现能力。

4.避免过拟合：
（1）减少特征数：
a. 人工的方法，根据经验删除和保留
b. Model based，用模型的方法。
（2）Regularization，正则项或者正则化。正则会保留所有的特征，它会尽量让每个feature的影响达到比较小的程度。
代价函数：
要使整个代价函数的值最小，就要让theta3和theta4的值趋向最小，让特征趋向于0。
正则会让theta变小，就不会过拟合。 过拟合就是因为特征过大。
一般，我们会惩罚所有的theta，通过lambda参数，它是权衡的作用，控制两个目标中的平衡关系，第一个是使假设更好地拟合训练数据，是训练误差达到最小，第二是要保持参数较小，在两者之前权衡，lambda的目的也是避免过拟合。

