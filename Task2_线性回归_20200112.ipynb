{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归 \n",
    "\n",
    "1.线性回归的原理  \n",
    "2.线性回归损失函数、代价函数、目标函数  \n",
    "3.优化方法(梯度下降法、牛顿法、拟牛顿法等)  \n",
    "4.sklearn参数详解  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、线性回归的原理 \n",
    "线性回归就是在N维空间中找一个形式像直线方程一样的函数来拟合数据而已。比如说，我们现在有这么一张图，横坐标代表房子的面积，纵坐标代表房价。\n",
    "然后呢，线性回归就是要找一条直线，并且让这条直线尽可能地拟合图中的数据点，这样子可以拟合初无数条直线，我们要从中找出最有的那条。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、线性回归损失函数、代价函数、目标函数  \n",
    "损失函数(Loss Function)：度量单样本预测的错误程度，损失函数值越小，模型就越好。  \n",
    "代价函数(Cost Function)：度量全部样本集的平均误差。  \n",
    "目标函数(Object Function)：代价函数和正则化函数，最终要优化的函数。  \n",
    "常用的损失函数包括：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数等；常用的代价函数包括均方误差、均方根误差、平均绝对误差等。  \n",
    "\n",
    "既然代价函数已经可以度量样本集的平均误差，为什么还要设定目标函数？\n",
    "原因是这样：\n",
    "当模型复杂度增加时，有可能对训练集可以模拟的很好，但是预测测试集的效果不好，出现过拟合现象，这就出现了所谓的“结构化风险”。结构风险最小化即为了防止过拟合而提出来的策略，定义模型复杂度为$J(F)$，目标函数可表示为：\n",
    "\n",
    "$$\\underset{f\\in F}{min}\\, \\frac{1}{n}\\sum^{n}_{i=1}L(y_i,f(x_i))+\\lambda J(F)$$\n",
    "\n",
    "例如有以上6个房价和面积关系的数据点，可以看到，当设定$f(x)=\\sum_{j=0}^{5}\\theta_jx_j$时，可以完美拟合训练集数据，但是，真实情况下房价和面积不可能是这样的关系，出现了过拟合现象。当训练集本身存在噪声时，拟合曲线对未知影响因素的拟合往往不是最好的。 通常，随着模型复杂度的增加，训练误差会减少；但测试误差会先增加后减小。我们的最终目的时试测试误差达到最小，这就是我们为什么需要选取适合的目标函数的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、优化方法  \n",
    "### 1.梯度下降  \n",
    "线性回归怎么求解最佳的参数。函数模型的函数中\\theta的值，选出最有的函数H(X)，那么就要让代价函数达到最小。  \n",
    "这里的代价函数就是最小二乘法，微分的方法来求解最小二乘代价函数的最优值，找到一个最优值来表达代价函数的最小值。方法是梯度下降(gradient descent)，它是凸优化的一种方法。\n",
    "\n",
    "梯度下降的步骤，\n",
    "1. initial theta，我们先随机给$\\theta$一些初始值，先给一些很小的值，比如0.1，0.01，0.001，但是不要给0。  \n",
    "2. 然后不断更新$\\theta$。每次减去一个步长(learning rate),这是一个常数。不断更新，每次在上次\\theta的基础上减去一个步长。  \n",
    "3. 直到指定的更新次数，比如100次。或者直到收敛，收敛就是更新至值不再变化或变化非常小。那么这个时候\\theta j就是要求的参数值。  \n",
    "\n",
    "一般的话，SGD随机梯度下降，比如10000个样本里面随机挑选一个样本来更新值，然后再挑一个，……，直到指定次数或者收敛(convergency)就停止更新。  \n",
    "另外，要控制learning rate步长的大小， 不能太大或者太小，太小会搜索的非常慢，比较耗时，太大的话很有可能会错过最优值。我们一般会通过交叉验证的方法选一个合适的步长，一般设置的小一点，不要太大。  \n",
    "\n",
    "梯度下降的分类：  \n",
    "SGD(stochastic gradient descent)：for i to m，更新至指定次数或收敛时。  \n",
    "BGD(batch gradient descent)：每次更新n个样本，也是更新至指定次数或收敛时。比较耗时，每次更新量比较大。  \n",
    "Mini BGD：每次挑选n个样本更新，这里的 n < m,又放回或者无放回都可以，自己设定。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.最小二乘矩阵求解  \n",
    "   normal equations:  \n",
    "   矩阵计算的好处，不需要循环迭代，计算的开销非常非常小。最小二乘其实是一种最佳的选择，尝试一下。  \n",
    "   很多时候，我们做运算，循环迭代的方法比较耗时耗资源，所以转化成矩阵的运算是做更好的选择。\n",
    "$$ X = \\left[ \\begin{array} {cccc}\n",
    "(x^{(1)})^T\\\\\n",
    "(x^{(2)})^T\\\\\n",
    "\\ldots \\\\\n",
    "(x^{(n)})^T\n",
    "\\end{array} \\right] $$  \n",
    "损失函数写作\n",
    "$$J(\\theta)=\\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y)$$\n",
    "           $$=\\frac{1}{2}((X\\theta)^TX\\theta+Y^TY-2(X\\theta)^TY)$$  \n",
    "然后两边求导，\n",
    "$$\\frac{\\partial{J(\\theta)}}{\\partial\\theta}  = \\frac{1}{2}\\frac{\\partial}{\\partial\\theta} \\theta^TX^TX\\theta - \\frac{\\partial}{\\partial\\theta} \\theta^T X^TY\n",
    "$$  \n",
    "（注意一下矩阵的运算方法，回顾一下线代知识）  \n",
    "$$X^TX\\theta = X^TY$$\n",
    "\n",
    "$$\\theta = (X^TX)^{(-1)}X^TY\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、sklearn参数详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成数据\n",
    "import numpy as np\n",
    "#生成随机数\n",
    "np.random.seed(1234)\n",
    "x = np.random.rand(500,3)\n",
    "#构建映射关系，模拟真实的数据待预测值,映射关系为y = 4.2 + 5.7*x1 + 10.8*x2，可自行设置值进行尝试\n",
    "y = x.dot(np.array([4.2,5.7,10.8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、先尝试调用sklearn的线性回归模型训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计的参数值为：[ 4.2  5.7 10.8]\n",
      "R2:1.0\n",
      "预测值为: [85.2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 调用模型\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "# 训练模型\n",
    "lr.fit(x,y)\n",
    "print(\"估计的参数值为：%s\" %(lr.coef_))\n",
    "# 计算R平方\n",
    "print('R2:%s' %(lr.score(x,y)))\n",
    "# 任意设定变量，预测目标值\n",
    "x_test = np.array([2,4,5]).reshape(1,-1)\n",
    "y_hat = lr.predict(x_test)\n",
    "print(\"预测值为: %s\" %(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计的参数值为：[ 4.2         5.70000001 10.79999999]\n",
      "预测值为：[85.19999998]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LR_GD():\n",
    "    def __init__(self):\n",
    "        self.w = None \n",
    "        #self.n_iters = None\n",
    "    def fit(self,X,y,alpha=0.05,loss = 1e-10): # 设定步长为0.03，判断是否收敛的条件为1e-10\n",
    "        y = y.reshape(-1,1) #重塑y值的维度以便矩阵运算\n",
    "        [m,d] = np.shape(X) #自变量的维度\n",
    "        self.w = np.zeros((d)) #将参数的初始值定为0\n",
    "        tol = 1e5\n",
    "        self.n_iters = 0\n",
    "        #============================= show me your code =======================\n",
    "        while tol > loss:   #设置收敛条件\n",
    "            hypothesis_f = X.dot(self.w).reshape(-1,1) \n",
    "            theta = self.w + alpha*np.mean(X*(y - hypothesis_f),axis=0) #计算迭代的参数值\n",
    "            tol = np.sum(np.abs(theta - self.w))\n",
    "            self.w = theta\n",
    "            #self.n_iters += 1 #更新迭代次数\n",
    "         #============================= show me your code =======================\n",
    "    def predict(self, X):\n",
    "        # 用已经拟合的参数值预测新自变量\n",
    "        y_pred = X.dot(self.w)\n",
    "        return y_pred  \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    lr_gd = LR_GD()\n",
    "    lr_gd.fit(x,y)\n",
    "    print(\"估计的参数值为：%s\" %(lr_gd.w))\n",
    "    x_test = np.array([2,4,5]).reshape(1,-1)\n",
    "    print(\"预测值为：%s\" %(lr_gd.predict(x_test)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.最小二乘矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计的参数值：[ 4.2  5.7 10.8]\n",
      "预测值为: [85.2]\n"
     ]
    }
   ],
   "source": [
    "class LR_LS():\n",
    "    def __init__(self):\n",
    "        self.w = None      \n",
    "    def fit(self, X, y):\n",
    "        # 最小二乘法矩阵求解\n",
    "        #============================= show me your code =======================\n",
    "        self.w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)    #np.linalg.inv 矩阵求逆\n",
    "        #============================= show me your code =======================\n",
    "    def predict(self, X):\n",
    "        # 用已经拟合的参数值预测新自变量\n",
    "        #============================= show me your code =======================\n",
    "        y_pred = X.dot(self.w)\n",
    "        #============================= show me your code =======================\n",
    "        return y_pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lr_ls = LR_LS()\n",
    "    lr_ls.fit(x,y)\n",
    "    print(\"估计的参数值：%s\" %(lr_ls.w))\n",
    "    x_test = np.array([2,4,5]).reshape(1,-1)\n",
    "    print(\"预测值为: %s\" %(lr_ls.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
